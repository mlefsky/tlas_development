import numpy as np
import matplotlib.pyplot as plt
import laspy
import pickle
import os
import subprocess
import rasterio
import rioxarray
import dask 
from dask.utils import SerializableLock
import datetime

# Nate asked what my code generally looks like, so I am sharing the 
# code I wrote to generate data products for Alaska.  The details of the
# processing steps are less important that the overall system structure. 
# In designing this version of my system, I have assumed that PDAL and GDAL 
# will be important to the work, as will some standard python libraries 
# such as rasterio, laspy, etc.
#
# As you probably know, keeping track of 
# intermediate and final datasets and the methods used to create 
# them is a PITA. One often has 10's or even 100's of datasets 
# generated by analyses (some documented, usually with some that are not) 
# whose provinence is only known by the analyst. I do a lot of work 
# comparing the results of data analyses, so there are many files
# generated, e.g. for picking an optimal segmentation routine to identify 
# individual trees. 
#
# The system I developed helps the individual analyst keep track of 
# their work and also allows for a unique "chain-of-custody" to know 
# exactely what was to create various datasets. This allows us to be 
# confident that the results we are giving clients are the correct ones
# _and_ allows future analysts to see exactly what steps were used in
# processing, which can be one part of our strategy for showing that 
# Teren has addressed the problem of showing that our methods aren't tied 
# to individual personnel.
# 
# In the system I use, each step in the processing chain generates 
# intermediate and final data products that follow a standard file naming 
# system implemented by the routines themselves and returns a structure 
# with the name of the files and parameters used to create them.
#
# This is especially useful for this work because I anticipate that 
# much of our processing pipeline will (as I said above) involve running
# external programs. PDAL, for instance, can be acessed using a python
# api, but the documentation isn't very good and most users seem to go 
# the route of generarting json files and running then using the pdal 
# application in bash/cmd. In that case, we need to keep track of
# any json or .bash/.bat scripts being generated and the command used  
# to generate/process them. I do this by extending the filename 
# scheme to include the json and bash files being created and record 
# their names and locations in a structure returned from the function.
#
# All functions work off a single filename for either the base laz file
# or of an important derivative (e.g. DTM)
# That filename is passed to each function which modifies it  
# to include details of the "trial_id" and the type of data product 
# produced. The trial_id keyword allows you to assign a unique id
# to every series of data products so that it and any intermediate data products don't
# get confused with each other. The single "trial_id" keyword can 
# itself be used to store multiple pieces of information separated
# with delimeters. For segmentation the files might look like: 
#
#        tile_xx_xx_ttt_ttt_pdtm.tif
#
# where ttt_ttt summarizes the test being done and/or some set of parameters
# 
# All functions return a dict with 
#    a <list> of the resulting json/bash commands (in Popen style) 
#    a single text command dervied from the <list> formatted with " ".join(commands)
#    an out_filename indicating the file that was created (can be more than one)
#    a json file (if applicable) to send to pdal
#    a "type" code to indicate what analysis created the file (e.g. tlas_dtm,tlas_dsm,...)
#    a las_directory to show the base directory of the input and output files
#    a "trial_id" code (if applicable) indicating a code to keep data products and
#     analyses separate and identifiable. 
# 
# If, as with tlas_chm and tlas_cover, the function calls one or more other functions, 
# the structure returned from each functions is included with the top level return dictionary
# so there is a record of dependencies. So, for a calculation of the canopy height model (CHM)
# the output structure includes all the details of the CHM analysis, as well as all the
# details of the DSM and DTM processing as keys in the dictionary:
#
# cmd_list  <class 'list'>
# cmd       <class 'str'>
# script    <class 'str'>
# out_filename <class 'str'>
# json_file <class 'str'>
# type      <class 'str'>
# trial_id     <class 'str'>
# las_directory    <class 'str'>
# dtm_outstruct <class 'dict'>     <------------
# dsm_struct <class 'dict'>     <------------
# bash_filename <class 'str'>
# bach_command <class 'str'> command to be used with bash - used
#   to put both pdal and bash calls in the same form

# Note: I use the presence of "outstruct" in an outstruct key name to identify 
# output structure that are included as part of an analysis. As such, don't
# use keys with "outstruct" in them
#
# Eventually, the hierarchy of analysis functions would include data management tools 
# that 1) match table/vector/raster data sources and 2) tools to generate final statistics.  
# Statistical results are also stored in the heiarchy along with all the information to 
# identify the intent of the analysis via the trial_id keyword.
# Other than manual operations (which I avoid as much as possible), any
# library or package can be used as long as there is a python wrapper function that returns 
# the details of the analysis sources, methods, and output location. 
#q
# At present, the code I use has a high meatball coefficient- basically it's improvised
# code that isn't very sophisticated. I'm sure that there is a more elegant way to 
# implement these features in python but for now I needed to get results quickly and 
# demonstrate the basic idea of what I am looking for. I'm hoping that implementing this
# system will be seen as advantageous to Teren and that it will receive resources to 
# develop as general solution as possible (e.g. using decorators)
#
# TTD:  I'd like to have the Date/Time of analysis and run duration of each process
#       The Name and Last modification date/time of the code being run.
#	Run duration of analysis
#       For some products, we need to wait to get results from various routines and we should 
#       have code that watches for the end of a process and then moves on.  
#   Determine dependencies to allow multiple steps at the same time rathewr than
#   wait until each job is done in order.
# More....

# pdtm = PDAL DTM
# pdsm = PDAL DSM
# and so on....

###########################################################################################
# Helper Functions
###########################################################################################

def return_res(filename):
    tmp=rasterio.open(filename)
    res=tmp.transform()
    res=res[0]
    return res

def clean_dirname(dirname):
    dirname_out=dirname
    if dirname[-1] != "/":  
        dirname_out=dirname_out+"/"
    return(dirname_out)

def clean_filename_parts(filename,dirname):
    filename_out=filename.replace(dirname,"")
    if filename_out[-1] == "_":  
        filename_out= filename_out[:len(filename_out)-1]
    if filename_out[-1] == "_":  
        filename_out= filename_out[:len(filename_out)-1]
    return(filename_out)

def current_time():
    now = datetime.datetime.now() # current date and time
    date_time = now.strftime("%Y%m%d:%H%M%S")
    return(date_time)
###########################################################################################
# Main Functions
###########################################################################################

def tlas_pdtm(las_filename_in,las_directory_in,trial_id_in,res,**kwargs):
# Makes a dtm from a las file using the PDAL smrf alogrithm
# Primarily used for comparing multiple processing techniques without 
# needing to produce multiple DTMs that are optimized for "best"
# results
    las_directory=clean_dirname(las_directory_in)
    las_filename=clean_filename_parts(las_filename_in,las_directory)
    las_filename=las_directory+las_filename
    trial_id=clean_filename_parts(trial_id_in,las_directory)

    out_tif_filename=las_filename.replace(trial_id,"").replace(".tif",trial_id+".tif")
    
#####################################################################################################
    cmds=\
"""                                                        
{
            "pipeline": [
{
                "type": "readers.las",
                "filename": "$1"
            },
            {
                "type":"filters.assign",
                "assignment":"Classification[:]=0"
            },
            {
                "type":"filters.elm"
            },
            {
                "type":"filters.outlier",
                "method":"statistical",
                "mean_k":8,
                "multiplier":3.0
            },
            {
                "type":"filters.smrf",
                "window":18,
                "threshold":0.30,
                "scalar":0.95
            },
            {
                "type":"filters.range",
                "limits":"Classification[2:2]"
            },
            {
                "type": "writers.gdal",
                "filename": "$2",
                "output_type": "mean",
                "gdaldriver": "GTiff",
                "resolution": $res, 
                "binmode":true,
                "nodata":"-9999",
                "radius": $res,
                "data_type": "float32"
            }

            ]
    }
"""
#####################################################################################################

    
    dtm_out_filename=las_filename.replace(".las",trial_id+"_pdtm.tif").replace(".laz",trial_id+"_pdtm.tif")
    cmds=cmds.replace("$1",las_filename)
    cmds=cmds.replace("$2",dtm_out_filename)
    cmds=cmds.replace("$res",str(res))
    cmds=cmds.replace("\\","\\\\")

    json_filename=las_directory+'tlas_dtm'+trial_id+'.json'
    #print(">>>>>>>>>>>>>",json_filename)
    with open(json_filename, 'w') as f:
        f.write(cmds)
    cmd=["pdal", "pipeline",las_directory+"tlas_dtm"+trial_id+".json"] 
    bash_command=" ".join(cmd)

    out={"cmd_list":cmd,"cmd":"".join(cmd),"out_filename":dtm_out_filename,
         "json_file":json_filename,"type":"dtm","las_filename":las_filename,
         "las_directory":las_directory,"script_runtime":current_time(),"res":res,
         "bash_command":bash_command}
    return(out)

def tlas_pdsm(las_filename_in,las_directory_in,trial_id_in,res,**kwargs):
    las_directory=clean_dirname(las_directory_in)
    las_filename=clean_filename_parts(las_filename_in,las_directory)
    las_filename=las_directory+las_filename
    trial_id=clean_filename_parts(trial_id_in,las_directory)
    out_filename=las_filename.replace(".laz",trial_id+'_pdsm.tif').replace(".las",trial_id+'_pdsm.tif')
    las_filename=las_filename.replace(trial_id,"")
     
      
###############################################################################
    cmds="""
{
    "pipeline":[
        {
            "type": "readers.las",
            "filename": "$1"},
        {
            "type":"filters.range",
            "limits":"returnnumber[0:1]"
        },

        {
            "type": "writers.gdal",
            "filename":"$2",
            "output_type":"mean",
            "gdaldriver":"GTiff",
            "resolution": $res,
            "radius": $res,
            "data_type": "float32",
            "binmode":true,
            "nodata":"-9999"

        }
    ]
}
"""

###############################################################################

    cmds=cmds.replace("$1",las_filename)
    cmds=cmds.replace("$2",out_filename)
    cmds=cmds.replace("$res",str(res))
    cmds=cmds.replace("\\","\\\\")
 
    with open(las_directory+'tlas_dsm'+trial_id+'.json', 'w') as f:
      f.write(cmds)
    bash_command="pdal pipeline "+las_directory+'tlas_dsm'+trial_id+'.json'
#    #print(">>>>>>>>",out_filename)
    out={"cmd_list":cmds,"cmd":" ".join(cmds),"out_filename":out_filename,
         "json_file":las_directory+"tlas_dsm"+trial_id+".json","type":"dsm","trial_id":trial_id,
         "las_directory":las_directory,"script_runtime":current_time(),"res":res,
         "bash_command":bash_command}
#    for k in out:
#        #print(k,out[k])

    return(out)


def tlas_chm(las_filename_in,las_directory_in,trial_id_in,in_res,hlimits=None,
             read_dtm="",read_dsm=""):

    las_directory=clean_dirname(las_directory_in)
    las_filename=clean_filename_parts(las_filename_in,las_directory)
    trial_id=clean_filename_parts(trial_id_in,las_directory)
    res_dtm=0
    res_dsm=0
    res=0

    if(read_dtm != ""):        
        dtm_outstruct=tlas_read(read_dtm,las_directory)
        dtm_out_filename=las_directory+dtm_outstruct['out_filename']
        res_dtm=dtm_outstruct['res']
        #print("TRUE1",dtm_out_filename,dtm_outstruct['out_filename'])


    if(read_dsm != ""):        
        dsm_outstruct=tlas_read(las_filename,las_directory)
        dsm_out_filename=dsm_outstruct['out_filename']
        res_dsm=dsm_outstruct['res']
        #print("TRUE2",dsm_out_filename) 
        
    if (res_dtm != 0 and res_dsm != 0):
        if res_dtm != res_dsm:
            print("dtm and dsm are read from existing files with different resolutions")
        if (res_dtm != 0) and (res_dsm ==0):
            res=res_dtm
        if (res_dtm == 0) and (res_dsm !=0):
            res=res_dsm

    if res == 0:
        res=in_res

    if read_dtm == "":
        dtm_outstruct=tlas_pdtm(las_filename,las_directory,trial_id,res)
        dtm_out_filename=dtm_outstruct['out_filename']

    if read_dsm == '':        
        dsm_outstruct=tlas_pdsm(las_filename,las_directory,trial_id,res)
        dsm_out_filename=dsm_outstruct['out_filename']


#    print("TMP@",dtm_out_filename)
    chm_out_filename= dtm_out_filename.replace("_pdtm",trial_id+"_chm").replace("_dtm",trial_id+"_chm")
#    print("TMP@",dsm_out_filename)
#    print("TMP@",chm_out_filename)
#    print("TMP@",trial_id)

#####################################################################################################
    if (hlimits == None):
        func='"B-A"'
    else: 
        func='\"numpy.clip(B-A,'+str(hlimits[0])+","+str(hlimits[1])+")\""
#####################################################################################################
    
    cmd=['gdal_calc.py',' -A '+dtm_out_filename,' -B '+dsm_out_filename,' --overwrite --calc='+func+ ' --outfile '+chm_out_filename+" --extent=intersect"]
    print("")
#    print(" ".join(cmd))
    bash_filename=las_directory+'tlas_chm'+trial_id+'.bash'

#    with open(bash_filename, 'w') as f:
#        for ix,c in enumerate(cmds):
#            f.write(c+"\n")
#            print(c)
#            print("\n")
#        f.write(" ".join(cmd))
#        #print(" ".join(cmd))

    bash_command=" ".join(cmd)

    out={"cmd_list":cmd,"cmd":"".join(cmd),"script":"","out_filename":las_directory+dtm_out_filename,
        "json_file":"","type":"chm","trial_id":trial_id,"las_filename":las_filename,
        "las_directory":las_directory,"dtm_outstruct":dtm_outstruct,
        "dsm_outstruct":dsm_outstruct,"bash_command":bash_command,"script_runtime":current_time(),
        "bash_filename":bash_filename}
    
    return(out)
    
def tlas_pdensity(las_filename,las_directory,trial_,res,limits=None,limit_code="",read_dtm="",*kwargs):

    if read_dtm == "":
        dtm_filename=las_filename.replace(".las",trial_+"_pdtm.tif").replace(".laz",trial_+"_pdtm.tif")
    else:
        dtm_filename=read_dtm
#        dtm_filename=read_dtm.replace('.tif',trial_+".tif")
#    print(">>>",dtm_filename)

    if len(limit_code) != 0:
        if limit_code[1] != "":
            limit_code="_"+limit_code
#    print('limit_code: '+limit_code)
#    print(type(limit_code),type(trial_),type(las_directory))
    out_filename=las_filename.replace(".las","_density"+limit_code+trial_+".tif").\
    replace(".laz","_density"+str(limit_code)+str(trial_)+".tif")
    
    #print("???????????",dtm_filename)
    cmds=[]
#####################################################################################################
    cmd_1="""
{  "pipeline":[
    {
        "filename":"$1"
    },\n"""

    cmds.append(cmd_1.replace("$1",las_directory+las_filename))
#####################################################################################################
    if (limits!=None):
        cmd_2="""{ "type":"filters.hag_dem",
            "raster":"$2"
        },\n{"type":"filters.range",
        """        
        cmd_2=cmd_2+"\"limits\":\"HeightAboveGround["+str(limits[0])+":"+str(limits[1])+"]\"},"
        cmds.append(cmd_2.replace("$2",las_directory+dtm_filename))
        
#####################################################################################################
    cmd_3=\
"""
{
        "type":"writers.gdal",
        "filename":"$3",        
        "dimension":"Z",
        "data_type":"float32",
        "output_type":"count",
        "resolution": $res,
        "binmode":true}]}
""" 
#####################################################################################################
    cmd_3=cmd_3.replace("$3",las_directory+out_filename)
    cmd_3=cmd_3.replace("$res",str(res))
    cmds.append(cmd_3)
    
    json_filename=las_directory+'mk_density_'+limit_code+"_"+trial_+'.json'
    json_filename=json_filename.replace("__","_")
    json_filename=json_filename.replace("__","_")
    bash_script=""
    out={"cmd_list":cmds,"cmd":" ".join(cmds),"out_filename":out_filename,
        "json_file":json_filename,"type":"density_"+limit_code,"trial_":trial_,
        "las_directory":las_directory,"filename":las_filename,"bash_cmd":bash_script,"limit_code":limit_code,
        "bash_command":"pdal pipeline "+json_filename,res:res}
    print(">>>>>>>>>>>>>",json_filename)
    with open(json_filename, 'w') as f:
        for c in cmds:
            f.write(c)
            print(">",c)

#    print(">>>>",json_filename)
    return(out)


def tlas_cover(las_filename,dtm_filename,las_directory,trial_,res=1.3):
    cmds=[]
    pdensity_struct=tlas_pdensity(las_filename,las_directory,trial_,res,read_dtm=dtm_filename,limits=None)
    print("><><><><><",las_filename,las_directory,trial_,res,dtm_filename)
#    cmds.append("pdal pipeline "+
#                pdensity_struct['json_file'])
    pdensity_lt1_struct=tlas_pdensity(las_filename,
            las_directory,trial_,res,read_dtm=dtm_filename,limits=[-2,1],limit_code="lt1")
#    cmds.append("pdal pipeline "+
#        pdensity_lt1_struct['json_file'])
    filename_A=las_directory+pdensity_struct['out_filename']
    filename_B=pdensity_lt1_struct['out_filename']
    ofilename=las_directory+filename_B.replace('.tif','_cover.tif').replace("_lt1","").replace("_density","")

    i=rasterio.open(las_directory+dtm_filename)
    bb=i.bounds
    #print(bb)
    left=bb[0]
    bottom=bb[1]
    right=bb[2]
    top=bb[3]
    projwin="--projwin "+str(left)+" "+str(top)+" "+str(right)+" "+str(bottom)+ " "
    #print("PW: ",projwin)

#####################################################################################################
    cmd=['gdal_calc.py','--quiet ','-A ',filename_A,
         ' -B ',filename_B,' --overwrite --calc="numpy.clip((B/A),0,1)" ',' --outfile '+ofilename,projwin+" --type='Float32'"]
#         ' -B ',filename_B,' --overwrite --calc="numpy.clip((B/A),0,1)" ',' --outfile '+ofilename,"--extent=intersect  --type='Float32'"]
#####################################################################################################

    #print(">>>>>>>>>>>>>>>>>",cmd)
    cmds.append(" ".join(cmd))

    bash_filename=las_directory+'mk_cover'+trial_+'.bash'


#    with open(bash_filename, 'w') as f:
#        for ix,c in enumerate(cmds):
#            f.write(c+"\n")
#            print(c)
#            print("\n")

    bash_command=" ".join(cmds)
    out=""

    out={"cmd_list":"","cmd":"","script":cmds,"out_filename":ofilename,
        "json_file":"","type":"cover","trial_":trial_,
        "las_directory":las_directory,"pdensity_outstruct":pdensity_struct,
        "pdensity_lt1_outstruct":pdensity_lt1_struct,"bash_filename":bash_filename,
        "bash_command":bash_command}

    return (out)


def tlas_read(new_filename,directory_in):  
#    print("new",new_filename)
    directory=clean_dirname(directory_in)
    out_filename= new_filename#.replace(".las","_pdtm.tif").replace(".laz","_pdtm.tif")
    dataset = rasterio.open(directory_in+new_filename)
    transform=dataset.transform
#    print("tr",transform)
#Affine(30.0, 0.0, 358485.0,,,
#       0.0, -30.0, 4265115.0)

    out={"cmd_list":"","cmd":"","script":"","out_filename":out_filename,
    "json_file":"","type":"external_file_read","trial_id":"","script_runtime":current_time(),
    "las_directory":directory,"las_filename":new_filename,"res":transform[0],
    "bash_command":"# external file read"}
    return(out)

def collect_bash_commands(dictionary: dict,out,verbose=False):
    for key in dictionary:
        value = dictionary[key]
        if "outstruct" in key:
            dummy=collect_bash_commands(value,out)
        else:
            if key=="bash_command":
                out.append(value)
#                print(value)
                return

#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================


def image_to_table(image_fname,skip1,csv_fname):
    import geopandas
    import pandas as pd
    import rasterio
    import matplotlib.pyplot as plt
    from shapely import points
    from rasterio.plot import show
    import numpy as np
    import glob

    ds = rasterio.open(image_fname, "r") #Read the raster
    a = ds.read(1) #Read as a numpy array

    height, width = a.shape #Find the height and width of the array

    cols, rows = np.meshgrid(np.arange(width), np.arange(height)) 

    xs, ys = rasterio.transform.xy(ds.transform, rows, cols) 

    xcoords = np.array(xs)
    ycoords = np.array(ys)

    unique, counts = np.unique(a, return_counts=True)
    counts = dict(zip(unique, counts))

    xindex, yindex = np.where(a==1)

    pts=points(xcoords,y=ycoords)
    pts2=pts.reshape(-1)
    skip=int(np.round(((len(pts2))/skip1)))
    pts3=pts2[::skip]

    gdf = geopandas.GeoDataFrame(list(range(len(pts3))), geometry=pts3)
    coord_list = [(x, y) for x, y in zip(gdf["geometry"].x, gdf["geometry"].y)]

    ds=rasterio.open(image_fname)
    src_io=np.squeeze(ds.read().astype(rasterio.float32))
    gdf[band_name] = [float(x) for x in ds.sample(coord_list)]
               
    
    df = pd.DataFrame(gdf)
    df.to_csv(out_image.replace(".tif",".csv"), index=False)


    
    return gdf

def merge_files_stub():
    with rasterio.open("/home/lefsky/time_trials/"+file_list[1]) as src0:
#    with rasterio.open("/home/lefsky/time_trials/tile_66_136_sub_01_chm.tif") as src0:
        meta = src0.meta

    # Update meta to reflect the number of layers
    meta.update(count = len(file_list))
    
    print(">>>>>>>>>>>>>")
    # Read each layer and write it to stack
    with rasterio.open(ofilename, 'w', **meta) as dst:
        for id, layer in enumerate(file_list, start=1):
            
            print(layer)
            with rasterio.open("/home/lefsky/time_trials/"+layer) as src1:
                dst.write_band(id, src1.read(1))
                dst.set_band_description(id,band_names[id])
    
    with open("/home/lefsky/time_trials/alltif.txt","r") as file:
        filenames=file.readlines()

    layer_stack(filenames,"/home/lefsky/time_trials/tile_66_137_stack.tif")
    #start=time.time(3P
    return()
    for ix in range(1,5):    
        print(ix)
        # setup files for two runs of tlas_cover and tlas_chm
        if (ix==1):
            las_directory="/home/lefy/time_trials/"
            trial_id="_sub_01"
            filename="tile_66_136_sub_01.laz"
            dtm_filename="tile_66_136_sub_01_pdtm.tif"
        if (ix==2):
            las_directory="/home/lefsky/time_trials/"
            trial_id="_sub_02"
            filename="tile_66_136_sub_02.laz"
            dtm_filename="tile_66_136_sub_02_pdtm.tif"
        if (ix==3):
            las_directory="/home/lefsky/time_trials/"
            trial_id="_sub_03"
            filename="tile_66_136_sub_03.laz"
            dtm_filename="tile_66_136_sub_03.pdtm.tif"

        if (ix==4):
            las_directory="/home/lefsky/time_trials/"
            trial_id="_sub_04"
            filename="tile_66_136_sub_04.laz"
            dtm_filename="tile_66_136_sub_04_pdtm.tif"
    
    #    generic_outstruct=tlas_chm(filename,las_directory,trial_id,1.3,hlimits=[0,60],read_dtm=dtm_filename)
        generic__outstruct=tlas_cover(filename,dtm_filename,las_directory,trial_id)
    #    print(generic_outstruct)
        outfile_bash=filename.replace('.las',trial_id+'.bash').replace('.laz',trial_id+'.bash')
    
        out=[]
        collect_commands=collect_bash_commands(generic__outstruct,out)
        #print("out",out)
    
        with open(outfile_bash, 'w') as f:  
            for o in out: 
                f.write(o+"\n")
                print(">>>>>>",o)


# transform rasterio plot to real world coords
#extent = [src.bounds[0], src.bounds[2], src.bounds[1], src.bounds[3]]
#ax = rasterio.plot.show(src, extent=extent, ax=ax, cmap="pink")

#gdf.plot(ax=ax)

#coord_list = [(x, y) for x, y in zip(gdf["geometry"].x, gdf["geometry"].y)]

#gdf = geopandas.GeoDataFrame([1, 2, 3, 4], geometry=points, crs=32630)


def layer_stack(file_list,ofilename):

    file_list=["tile_66_136_sub_01_sub_01_cover.tif",
    "tile_66_136_sub_01_density_lt1_sub_01.tif",
    "tile_66_136_sub_01_density_sub_01.tif",
    "tile_66_136_sub_01_pdsm.tif",
    "tile_66_136_sub_01_pdtm.tif",
    "tile_66_136_sub_02_chm.tif",
    "tile_66_136_sub_02_sub_02_cover.tif",
    "tile_66_136_sub_02_density_lt1_sub_02.tif",
    "tile_66_136_sub_02_density_sub_02.tif",
    "tile_66_136_sub_02_pdsm.tif",
    "tile_66_136_sub_02_pdtm.tif",
    "tile_66_136_sub_03_chm.tif",
    "tile_66_136_sub_03_sub_03_cover.tif",
    "tile_66_136_sub_03_density_lt1_sub_03.tif",
    "tile_66_136_sub_03_density_sub_03.tif",
    "tile_66_136_sub_03_pdsm.tif",
    "tile_66_136_sub_03.pdtm.tif",
    "tile_66_136_sub_04_chm.tif",
    "tile_66_136_sub_04_sub_04_cover.tif",
    "tile_66_136_sub_04_density_lt1_sub_04.tif",
    "tile_66_136_sub_04_density_sub_04.tif",
    "tile_66_136_sub_04_pdsm.tif",
    "tile_66_136_sub_04_pdtm.tif"]

    band_names=["sub_01_chm",
    "sub_01_cover",
    "sub_01_density_lt1",
    "sub_01_density",
    "sub_01_pdsm",
    "sub_01_pdtm",
    "sub_02_chm",
    "sub_02_cover",
    "sub_02_density_lt1",
    "sub_02_density",
    "sub_02_pdsm",
    "sub_02_pdtm",
    "sub_03_chm",
    "sub_03_cover",
    "sub_03_density_lt1",
    "sub_03_density",
    "sub_03_pdsm",
    "sub_03pdtm",
    "sub_04_chm",
    "sub_04_cover",
    "sub_04_density_lt1",
    "sub_04_density",
    "sub_04_pdsm",
    "sub_04_pdtm"]


    # Read metadata of first file

    
    return()

#    str1="/home/lefsky/time_trials/tile_66_136_sub_01_chm.tif"

#    str2="/home/lefsky/time_trials/"+file_list[0]    

#    for ix in range(len(str1)):
#        print(str1[ix],str2[ix],str1[ix]==str2[ix])
#    print("")



#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================

def main():   
    print(image_to_table(las_dire
                         ctory+"tile_66_137_stack.tif",1000,las_directory+"tile_66_137_stack.csv"))


if __name__ == "__main__":  
    # execute only if run as a scrip
    main()