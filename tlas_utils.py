from dask.utils import SerializableLock
import numpy as np
import matplotlib.pyplot as plt
import laspy
import pickle
import os
import subprocess
import rasterio
import rioxarray

# Nate asked what my code generally looks like, so I am sharing the 
# code I wrote to generate data products for Alaska.  The details of the
# processing steps are less important that the overall system structure. 
# In designing this version of my system, I have assumed that PDAL and GDAL 
# will be important to the work, as will some standard python libraries 
# such as rasterio, laspy, etc.
#
# As you probably know, keeping track of 
# intermediate and final datasets and the methods used to create 
# them is a PITA. One often has 10's or even 100's of datasets 
# generated by analyses (some documented, usually with some that are not) 
# whose provinence is only known by the analyst. I do a lot of work 
# comparing the results of data analyses, so there are many files
# generated, e.g. for picking an optimal segmentation routine to identify 
# individual trees. 
#
# The system I developed helps the individual analyst keep track of 
# their work and also allows for a unique "chain-of-custody" to know 
# exactely what was to create various datasets. This allows us to be 
# confident that the results we are giving clients are the correct ones
# _and_ allows future analysts to see exactly what steps were used in
# processing, which can be one part of our strategy for showing that 
# Teren has addressed the problem of showing that our methods aren't tied 
# to individual personnel.
# 
# In the system I use, each step in the processing chain generates 
# intermediate and final data products that follow a standard file naming 
# system implemented by the routines themselves and returns a structure 
# with the name of the files and parameters used to create them.
#
# This is especially useful for this work because I anticipate that 
# much of our processing pipeline will (as I said above) involve running
# external programs. PDAL, for instance, can be acessed using a python
# api, but the documentation isn't very good and most users seem to go 
# the route of generarting json files and running then using the pdal 
# application in bash/cmd. In that case, we need to keep track of
# any json or .bash/.bat scripts being generated and the command used  
# to generate/process them. I do this by extending the filename 
# scheme to include the json and bash files being created and record 
# their names and locations in a structure returned from the function.
#
# All functions work off a single filename for either the base laz file
# or of an important derivative (e.g. DTM)
# That filename is passed to each function which modifies it  
# to include details of the "trial" and the type of data product 
# produced. The trial keyword allows you to assign a unique id
# to every series of data products so that it and any intermediate data products don't
# get confused with each other. The single "trial" keyword can 
# itself be used to store multiple pieces of information separated
# with delimeters. For segmentation the files might look like: 
#
#        tile_xx_xx_ttt_ttt_pdtm.tif
#
# where ttt_ttt summarizes the test being done and/or some set of parameters
# 
# All functions return a dict with 
#    a <list> of the resulting json/bash commands (in Popen style) 
#    a single text command dervied from the <list> formatted with " ".join(commands)
#    an out_filename indicating the file that was created (can be more than one)
#    a json file (if applicable) to send to pdal
#    a "type" code to indicate what analysis created the file (e.g. mk_dtm,mk_dsm,...)
#    a prefix to show the base directory of the input and output files
#    a "trial" code (if applicable) indicating a code to keep data products and
#     analyses separate and identifiable. 
# 
# If, as with mk_chm and mk_cover, the function calls one or more other functions, 
# the structure returned from each functions is included with the top levelreturn dictionary
# so there is a record of dependencies. So, for a calculation of the canopy height model (CHM)
# the output structure includes all the details of the CHM analysis, as well as all the
# details of the DSM and DTM processing as keys in the dictionary:
#
# cmd_list  <class 'list'>
# cmd       <class 'str'>
# script    <class 'str'>
# out_filename <class 'str'>
# json_file <class 'str'>
# type      <class 'str'>
# trial     <class 'str'>
# prefix    <class 'str'>
# dtm_struct <class 'dict'>     <------------
# dsm_struct <class 'dict'>     <------------
# bash_filename <class 'str'>
#
# Eventually, the hierarchy of analysis functions would include data management tools 
# that 1) match table/vector/raster data sources and 2) tools to generate final statistics.  
# Statistical results are also stored in the heiarchy along with all the information to 
# identify the intent of the analysis via the trial keyword.
# Other than manual operations (which I avoid as much as possible), any
# library or package can be used as long as there is a python wrapper function that returns 
# the details of the analysis sources, methods, and output location. 
#
# At present, the code I use has a high meatball coefficient- basically it's improvised
# code that isn't very sophisticated. I'm sure that there is a more elegant way to 
# implement these features in python but for now I needed to get results quickly and 
# demonstrate the basic idea of what I am looking for. I'm hoping that implementing this
# system will be seen as advantageous to Teren and that it will receive resources to 
# develop as general solution as possible (e.g. using decorators)
#
# TTD:  I'd like to have the Date/Time of analysis and run duration of each process
#       The Name and Last modification date/time of the code being run.
#       For some products, we need to wait to get results from various routines and we should 
#       have code that watches for the end of a process and then moves on.  
# 
# More....

# pdtm = PDAL DTM
# pdsm = PDAL DSM
# and so on....

def mk_pdtm(filename,prefix,trial,**kwargs):
    filename=prefix+filename
    out_filename=prefix+filename.replace(trial,"").replace(".tif",trial+".tif")
    
#####################################################################################################
    cmds="""
{
            "pipeline": [
            {
                "type": "readers.las",
                "filename": "$1"
            },
            {
                "type":"filters.assign",
                "assignment":"Classification[:]=0"
            },
            {
                "type":"filters.elm"
            },
            {
                "type":"filters.outlier",
                "method":"statistical",
                "mean_k":8,
                "multiplier":3.0
            },
            {
                "type":"filters.smrf",
                "window":18,
                "threshold":0.30,
                "scalar":0.95
            },
            {
                "type":"filters.range",
                "limits":"Classification[2:2]"
            },
            {
                "type": "writers.gdal",
                "filename": "$2",
                "output_type": "mean",
                "gdaldriver": "GTiff",
                "resolution": 1, 
                "binmode":true,
                "nodata":"-9999",
                "radius": 1,
                "data_type": "float32"
            }

            ]
    }"""
#####################################################################################################

    ofilename=filename.replace(trial,"").replace(".las",trial+"_pdtm.tif").replace(".laz",trial+"_pdtm.tif")
    cmds=cmds.replace("$1",filename)
    cmds=cmds.replace("$2",ofilename)
    cmds=cmds.replace("\\","\\\\")

    with open(prefix+'mk_dtm'+trial+'.json', 'w') as f:
      f.write(cmds)

    cmd=["pdal", "pipeline",prefix+"mk_dtm"+trial+".json"] 

    out={"cmd_list":cmd,"cmd":" ".join(cmd),"out_filename":ofilename,\
         "json_file":prefix+"mk_dtm"+trial+".json","type":"dtm",
         "prefix":prefix}

    return out

def mk_pdsm(filename,prefix,trial,**kwargs):
    filename=prefix+filename.replace(trial,"")
    out_filename=filename.replace('.laz',trial+'_pdsm.tif')

###############################################################################
    cmds="""
{
    "pipeline":[
        {
            "type": "readers.las",
            "filename": "$1"},
        {
            "type":"filters.range",
            "limits":"returnnumber[0:1]"
        },

        {
            "type": "writers.gdal",
            "filename":"$2",
            "output_type":"mean",
            "gdaldriver":"GTiff",
            "resolution": 1,
            "radius": 1,
            "data_type": "float32",
            "nodata":"-9999"

        }
    ]
}
"""

###############################################################################

    cmds=cmds.replace("$1",filename)
    cmds=cmds.replace("$2",out_filename)
    cmds=cmds.replace("\\","\\\\")

    with open(prefix+'mk_dsm'+trial+'.json', 'w') as f:
      f.write(cmds)

    out={"cmd_list":cmds,"cmd":" ".join(cmds),"out_filename":out_filename,\
         "json_file":prefix+"mk_dsm"+trial+".json","type":"dsm","trial":trial,\
         "prefix":prefix}

    return(out)


def mk_chm(filename1,prefix,trial,hlimits=None):
    filename=filename1.replace(prefix,"")
    dtm_struct=mk_pdtm(filename,prefix,trial)
    dsm_struct=mk_pdsm(filename,prefix,trial)
    ofilename=dtm_struct['out_filename'].replace('pdtm','pchm')
#####################################################################################################
    if (hlimits == None):
        func='"B-A"'
    else: 
        func='\"numpy.clip(B-A,'+str(hlimits[0])+","+str(hlimits[1])+")\""
#####################################################################################################
    
    cmd=['gdal_calc.py','-A '+dtm_struct['out_filename'],'-B '+dsm_struct['out_filename'],' --overwrite --calc='+func+ ' --outfile '+ofilename+" --extent=union"]

    bash_filename=prefix+'mk_chm'+trial+'.bash'

    with open(bash_filename, 'w') as f:
#        for ix,c in enumerate(cmds):
##            f.write(c+"\n")
#            print(c)
#            print("\n")
        f.write(" ".join(cmd))
        print(" ".join(cmd))

    out={"cmd_list":cmd,"cmd":" ".join(cmd),"script":"","out_filename":prefix+ofilename,
        "json_file":"","type":"chm","trial":trial,
        "prefix":prefix,"dtm_struct":dtm_struct,
        "dsm_struct":dsm_struct,"bash_filename":bash_filename}

    return (out)

def mk_pdensity(filename,prefix,trial,limits=None,limit_code="",**kwargs):

    dtm_filename=filename.replace(".las","_pdtm.tif").replace(".laz","_pdtm.tif")
    if (limit_code != ""):
        limit_code="_"+limit_code
    out_filename=filename.replace(".las","_density_"+limit_code+".tif").replace(".laz","_density"+limit_code+".tif")
    cmds=[]
#####################################################################################################
    cmd_1="""
{  "pipeline":[
    {
        "type":"readers.las",
        "filename":"$1"
    },"""
    cmds.append(cmd_1.replace("$1",filename))
#####################################################################################################

    if (limits!=None):
        cmd_2=\
"""{ "type":"filters.hag_dem",
    "raster":"$2"
},{"type":"filters.range",
"""        
        cmd_2=cmd_2+"\"limits\":\"HeightAboveGround["+str(limits[0])+":"+str(limits[1])+"]\"},"
        cmds.append(cmd_2.replace("$2",dtm_filename))

#####################################################################################################
    cmd_3=\
"""
{
        "type":"writers.gdal",
        "filename":"$3",        
        "dimension":"Z",
        "data_type":"float32",
        "output_type":"mean",
        "resolution": 1,
        "binmode":true}]}
""" 
#####################################################################################################
    cmd_3=cmd_3.replace("$3",out_filename)
    cmds.append(cmd_3)
    
    json_filename=prefix+'mk_density_'+limit_code+"_"+trial+'.json'
    json_filename=json_filename.replace("__","_")
    json_filename=json_filename.replace("__","_")
    bash_script=""
    out={"cmd_list":cmds,"cmd":" ".join(cmds),"out_filename":out_filename,\
        "json_file":json_filename,"type":"density_"+limit_code,"trial":trial,
        "prefix":prefix,"filename":filename,"bash_cmd":bash_script,"limit_code":limit_code}
        
    with open(json_filename, 'w') as f:
        for c in cmds:
            f.write(c)
#            print(">",c)

#    print(">>>>",json_filename)
    return(out)


def mk_cover(filename,prefix,trial):
    cmds=[]
    pdensity_struct=mk_pdensity(filename,prefix,trial,limits=None)
    cmds.append("conda activate pdal_261; pdal pipeline "+
                pdensity_struct['json_file'])
    pdensity_lt1_struct=mk_pdensity(filename,
            prefix,trial,limits=[-2,1],limit_code="lt1")
    cmds.append("conda activate pdal_261; pdal pipeline "+
        pdensity_lt1_struct['json_file'])
    filename_A=pdensity_struct['out_filename']
    filename_B=pdensity_lt1_struct['out_filename']
    ofilename=filename_B.replace('.tif','_cover.tif').replace("_lt1","").replace("_density","")

#####################################################################################################
    cmd=['gdal_calc.py','--quiet ','-A ',filename_A,
         ' -B ',filename_B,' --overwrite --calc="numpy.clip((B/A),0,1)" ',' --outfile '+ofilename,"--extent=union  --type='Float32'"]
#####################################################################################################

    cmds.append(" ".join(cmd))


    bash_filename=prefix+'mk_cover'+trial+'.bash'

    with open(bash_filename, 'w') as f:
        for ix,c in enumerate(cmds):
            f.write(c+"\n")
            print(c)
            print("\n")

    out=""

    out={"cmd_list":"","cmd":"","script":cmds,"out_filename":ofilename,
        "json_file":"","type":"cover","trial":trial,
        "prefix":prefix,"pdensity_struct":pdensity_struct,
        "pdensity_lt1_struct":pdensity_lt1_struct,"bash_filename":bash_filename}

    return (out)


#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
#==============================================================================
import time
#start=time.time()

# setup files for two runs of mk_cover and mk_chm
if (1==0):
    prefix="/home/lefsky/time_trials/"
    trial=""
    filename="tile_67_136.laz"
else:
    prefix="/home/lefsky/time_trials/"
    trial="_sub_2"
    filename="tile_67_136_sub_2.laz"

#print(mk_subsample("tile_67_136.laz",2,prefix))
dtm_cmd=mk_pdtm(filename,dtm_out_filename,prefix,trial,do_print=True)
dsm_cmd=mk_pdsm(filename,dsm_out_filename,prefix,trial,do_print=True)


#tmp=mk_cover(filename,prefix,trial)
tmp2=mk_chm(filename,prefix,trial,hlimits=[0,60])
for t in tmp2.keys():
#    print(t)
    print(t,type(tmp2[t]))



